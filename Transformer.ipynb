{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsl8yN5myOgVf+1VcY16QM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shirin1996/ENSF619.2_Assignments/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ub86MqcwyDM"
      },
      "source": [
        "import math  \n",
        "from typing import Tuple \n",
        "\n",
        "import torch \n",
        "from torch import nn, Tensor \n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ise-XAft4pX2"
      },
      "source": [
        "##**Define the model**\n",
        "\n",
        "we train a `nn.TransformerEncoder` model on a language modeling task. The language modeling task is to assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words. A sequence of tokens are passed to the embedding layer first, followed by a positional encoding layer to account for the order of the word (see the next paragraph for more details). The `nn.TransformerEncoder` consists of multiple layers of `nn.TransformerEncoderLayer`. Along with the input sequence, a square attention mask is required because the self-attention layers in `nn.TransformerEncoder` are only allowed to attend the earlier positions in the sequence. For the language modeling task, any tokens on the future positions should be masked. To produce a probability distribution over output words, the output of the nn.`TransformerEncoder` model is passed through a linear layer followed by a log-softmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXDObxXaw922"
      },
      "source": [
        "class TransformerModel(nn.Module): #bulding transformer from scratch \n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__() #inheret from nn.Module \n",
        "        self.model_type = 'Transformer' \n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout) #applying PositionalEncoding to depict the order of words\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout) #trasformer layers\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model) #input embedding \n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None: #weights of our model\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor: # the forward propogation function \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXAHcoB85o4c"
      },
      "source": [
        "`PositionalEncoding` module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use `sine`and `cosine` functions of different frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmHkuy2DyC4o"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggsmYoB6GV4"
      },
      "source": [
        "##**Load and batch data**\n",
        "\n",
        "Batching enables more parallelizable processing. However, batching means that the model treats each column independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldWRC4CE59KF"
      },
      "source": [
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1sfGZY_6q97"
      },
      "source": [
        "##**Functions to generate input and target sequence**\n",
        "\n",
        "`get_batch()` generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length `bptt`. For the language modeling task, the model needs the following words as `Target`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIEFmmRX6U0G"
      },
      "source": [
        "bptt = 10\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8_qFe0n686_"
      },
      "source": [
        "##**Initiate an instance**\n",
        "\n",
        "The model hyperparameters are defined below. The vocab size is equal to the length of the vocab object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRLifuEm64gK"
      },
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3eFLvX-7TLc"
      },
      "source": [
        "##**Run the model**\n",
        "We use `CrossEntropyLoss` with the *SGD (stochastic gradient descent)* optimizer. The learning rate is initially set to `5.0` and follows a StepLR schedule. During training, we use `nn.utils.clip_grad_norm_` to prevent gradients from exploding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMBfUQDQ7HLc"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(0)\n",
        "        if batch_size != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(0)\n",
        "            if batch_size != bptt:\n",
        "                src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm3A2A5P76nr"
      },
      "source": [
        "Finally loop over epochs. Save the model if the validation loss is the best we’ve seen so far. Adjust the learning rate after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMA_YkC271_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608f060f-11a8-43c7-df5e-9a211378184a"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 1\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/10249 batches | lr 5.00 | ms/batch 248.31 | loss  6.26 | ppl   524.58\n",
            "| epoch   1 |   400/10249 batches | lr 5.00 | ms/batch 249.52 | loss  6.09 | ppl   443.22\n",
            "| epoch   1 |   600/10249 batches | lr 5.00 | ms/batch 245.81 | loss  6.11 | ppl   452.47\n",
            "| epoch   1 |   800/10249 batches | lr 5.00 | ms/batch 244.96 | loss  6.10 | ppl   443.84\n",
            "| epoch   1 |  1000/10249 batches | lr 5.00 | ms/batch 244.81 | loss  6.15 | ppl   470.57\n",
            "| epoch   1 |  1200/10249 batches | lr 5.00 | ms/batch 244.74 | loss  6.14 | ppl   465.01\n",
            "| epoch   1 |  1400/10249 batches | lr 5.00 | ms/batch 246.46 | loss  6.13 | ppl   460.79\n",
            "| epoch   1 |  1600/10249 batches | lr 5.00 | ms/batch 247.02 | loss  6.02 | ppl   412.72\n",
            "| epoch   1 |  1800/10249 batches | lr 5.00 | ms/batch 245.37 | loss  6.10 | ppl   445.26\n",
            "| epoch   1 |  2000/10249 batches | lr 5.00 | ms/batch 244.55 | loss  5.99 | ppl   400.29\n",
            "| epoch   1 |  2200/10249 batches | lr 5.00 | ms/batch 244.22 | loss  6.07 | ppl   431.34\n",
            "| epoch   1 |  2400/10249 batches | lr 5.00 | ms/batch 243.94 | loss  6.08 | ppl   436.06\n",
            "| epoch   1 |  2600/10249 batches | lr 5.00 | ms/batch 243.09 | loss  6.04 | ppl   420.42\n",
            "| epoch   1 |  2800/10249 batches | lr 5.00 | ms/batch 242.92 | loss  6.11 | ppl   449.18\n",
            "| epoch   1 |  3000/10249 batches | lr 5.00 | ms/batch 243.56 | loss  6.04 | ppl   418.44\n",
            "| epoch   1 |  3200/10249 batches | lr 5.00 | ms/batch 243.72 | loss  6.06 | ppl   427.42\n",
            "| epoch   1 |  3400/10249 batches | lr 5.00 | ms/batch 243.49 | loss  6.03 | ppl   415.87\n",
            "| epoch   1 |  3600/10249 batches | lr 5.00 | ms/batch 244.26 | loss  6.09 | ppl   441.98\n",
            "| epoch   1 |  3800/10249 batches | lr 5.00 | ms/batch 243.29 | loss  6.07 | ppl   432.40\n",
            "| epoch   1 |  4000/10249 batches | lr 5.00 | ms/batch 244.15 | loss  6.10 | ppl   444.82\n",
            "| epoch   1 |  4200/10249 batches | lr 5.00 | ms/batch 242.96 | loss  6.14 | ppl   464.07\n",
            "| epoch   1 |  4400/10249 batches | lr 5.00 | ms/batch 243.59 | loss  6.12 | ppl   456.08\n",
            "| epoch   1 |  4600/10249 batches | lr 5.00 | ms/batch 244.60 | loss  6.14 | ppl   462.38\n",
            "| epoch   1 |  4800/10249 batches | lr 5.00 | ms/batch 243.07 | loss  6.05 | ppl   422.38\n",
            "| epoch   1 |  5000/10249 batches | lr 5.00 | ms/batch 243.15 | loss  6.13 | ppl   460.68\n",
            "| epoch   1 |  5200/10249 batches | lr 5.00 | ms/batch 243.30 | loss  6.17 | ppl   478.93\n",
            "| epoch   1 |  5400/10249 batches | lr 5.00 | ms/batch 242.79 | loss  6.12 | ppl   455.21\n",
            "| epoch   1 |  5600/10249 batches | lr 5.00 | ms/batch 243.60 | loss  6.14 | ppl   461.79\n",
            "| epoch   1 |  5800/10249 batches | lr 5.00 | ms/batch 242.55 | loss  6.12 | ppl   453.97\n",
            "| epoch   1 |  6000/10249 batches | lr 5.00 | ms/batch 242.64 | loss  6.17 | ppl   478.13\n",
            "| epoch   1 |  6200/10249 batches | lr 5.00 | ms/batch 241.94 | loss  6.22 | ppl   502.26\n",
            "| epoch   1 |  6400/10249 batches | lr 5.00 | ms/batch 242.20 | loss  6.31 | ppl   549.97\n",
            "| epoch   1 |  6600/10249 batches | lr 5.00 | ms/batch 241.83 | loss  6.21 | ppl   497.88\n",
            "| epoch   1 |  6800/10249 batches | lr 5.00 | ms/batch 243.01 | loss  6.25 | ppl   516.16\n",
            "| epoch   1 |  7000/10249 batches | lr 5.00 | ms/batch 242.86 | loss  6.25 | ppl   518.86\n",
            "| epoch   1 |  7200/10249 batches | lr 5.00 | ms/batch 242.85 | loss  6.23 | ppl   509.93\n",
            "| epoch   1 |  7400/10249 batches | lr 5.00 | ms/batch 242.66 | loss  6.15 | ppl   466.98\n",
            "| epoch   1 |  7600/10249 batches | lr 5.00 | ms/batch 242.41 | loss  6.17 | ppl   479.34\n",
            "| epoch   1 |  7800/10249 batches | lr 5.00 | ms/batch 242.89 | loss  6.22 | ppl   504.09\n",
            "| epoch   1 |  8000/10249 batches | lr 5.00 | ms/batch 242.42 | loss  6.26 | ppl   524.72\n",
            "| epoch   1 |  8200/10249 batches | lr 5.00 | ms/batch 242.29 | loss  6.25 | ppl   517.28\n",
            "| epoch   1 |  8400/10249 batches | lr 5.00 | ms/batch 242.25 | loss  6.22 | ppl   500.33\n",
            "| epoch   1 |  8600/10249 batches | lr 5.00 | ms/batch 242.03 | loss  6.27 | ppl   528.40\n",
            "| epoch   1 |  8800/10249 batches | lr 5.00 | ms/batch 242.58 | loss  6.30 | ppl   542.34\n",
            "| epoch   1 |  9000/10249 batches | lr 5.00 | ms/batch 242.22 | loss  6.25 | ppl   517.09\n",
            "| epoch   1 |  9200/10249 batches | lr 5.00 | ms/batch 241.69 | loss  6.18 | ppl   480.88\n",
            "| epoch   1 |  9400/10249 batches | lr 5.00 | ms/batch 242.22 | loss  6.21 | ppl   497.72\n",
            "| epoch   1 |  9600/10249 batches | lr 5.00 | ms/batch 241.73 | loss  6.24 | ppl   510.39\n",
            "| epoch   1 |  9800/10249 batches | lr 5.00 | ms/batch 242.27 | loss  6.25 | ppl   520.04\n",
            "| epoch   1 | 10000/10249 batches | lr 5.00 | ms/batch 241.38 | loss  6.19 | ppl   487.69\n",
            "| epoch   1 | 10200/10249 batches | lr 5.00 | ms/batch 241.94 | loss  6.15 | ppl   468.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2568.80s | valid loss  6.05 | valid ppl   425.03\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlRnduKt8RHS"
      },
      "source": [
        "##**Evaluate the best model on the test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_GaQehP8DkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1023b8db-4b69-44ed-ee2d-8a71a11e4b54"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.97 | test ppl   392.11\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GBDKT9_M43v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}